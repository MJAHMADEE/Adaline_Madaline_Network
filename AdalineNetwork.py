# -*- coding: utf-8 -*-
"""HW1_Q2_AdaLine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tYrR0jetN0HVAXV_v9i1zQIt0ZpPHvsE

# 2.1. AdaLine

## 2.1.A. Generate and Plot Data
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import random

np.random.seed(100)

avg_x_1 = 0
sd_x_1 = 0.1
Class1_x = np.random.normal(avg_x_1, sd_x_1, 100)

avg_y_1 = 0
sd_y_1 = 0.4
Class1_y = np.random.normal(avg_y_1, sd_y_1, 100)

Class1_Data = list(zip(Class1_x,Class1_y))
Class1_Label = np.ones(100)
Class1 = list(zip(Class1_Data,Class1_Label))

avg_x_2 = 1
sd_x_2 = 0.2
Class2_x = np.random.normal(avg_x_2, sd_x_2, 100)

avg_y_2 = 1
sd_y_2 = 0.2
Class2_y = np.random.normal(avg_y_2, sd_y_2, 100)

Class2_Data = list(zip(Class2_x,Class2_y))
Class2_Label = np.zeros(100) - np.ones(100)
Class2 = list(zip(Class2_Data,Class2_Label))

plt.scatter(Class1_x, Class1_y, c ="red", linewidths = 1)
plt.scatter(Class2_x, Class2_y, c ="blue", linewidths = 1)

plt.xlabel("x")
plt.ylabel("y")
plt.legend(["Class 1" , "Class 2"])
plt.grid(True)  # add grid
plt.savefig("Q21figure1.pdf")
plt.show()

"""## 2.1.B. AdaLine Implementation

"""

def PrepareData(Class1,Class2):
    random.seed(0)
    AllData = Class1 + Class2
    random.shuffle(AllData)
    df = pd.DataFrame (AllData, columns = ['datapoints','labels'])
    x = df['datapoints']
    y = df['labels']
    return x,y

# Step 0 
# Initialize weights & bias (*sm for small random values)
def RandomInitialize(sm):
    np.random.seed(100)
    w = np.random.rand(1,2) * sm
    b = np.zeros(1)
    print(f"Initial w={w}, Initial b={b}")
    return w,b

def sgn(net):
    if net >= 0:
        return 1
    else:
        return -1

def tanh(net):
    return np.tanh(net)

# Step 3
# Compute yin = net = w.x + b
def Forward(w,x,b,actfunc):
    net = np.dot(w,x) + b
    if actfunc == 'sgn':
        h = sgn(net)
    elif actfunc == 'tanh':
        h = tanh(net)
    return h, net

# Step 4
# Update - sgn
def DeltaUpdate(w,b,x,t,net,alpha):
    t = t.reshape(1,)
    x = x.reshape((1,2))
    delta_w = alpha * np.dot((t - net), x)
    delta_b = alpha * (t - net)
    w = w + delta_w
    b = b + delta_b
    return w,b

# Step 4
# Update - tanh
def tanhUpdate(w,b,x,t,h,alpha,gamma):
    t = t.reshape(1,)
    x = x.reshape((1,2))
    diff_w = alpha * (1-h**2) * gamma * np.dot((t - h), x)
    diff_b = alpha * (1-h**2) * gamma * (t - h)
    w = w + diff_w
    b = b + diff_b
    return w,b

# Step 5
# Compute Cost Function - sgn
def sgnCost(t,net):
    error = 0.5 * np.power((t-net),2)
    return error

# Step 5
# Compute Cost Function - tanh
def tanhCost(t,gamma,net):
    error = 0.5 * np.power( t - gamma * np.tanh(net),2) 
    return error

def SeparationLine(start,end,w,b):
    x= np.linspace(start,end)
    y = -(w[0][0] * x + b)
    y = y / w[0][1]
    return x,y

# Training Func
def Adaline(x,y,max_iter,learning_rate,actfunc,samples):
    cost_list = []
    eps = 0.00005 # End criteria
    sm = 0.01 # In order to make small weights
    gamma = 0.05  #Hyperparamter for tanh actfunc
    w,b = RandomInitialize(sm)          #Step 0 
    print("Hyperparams are: ",f"eps={eps}, max_iter={max_iter}, learning_rate={learning_rate}, actfunc={actfunc}, sm={sm}, gamma={gamma}")
    for i in range(max_iter):           #Step 1
        h, net = Forward(w,np.asarray(x[i%samples]),b,actfunc) #Step 3
        if actfunc == 'sgn':
            cost = sgnCost(np.asarray(y[i%samples]),net)
        elif actfunc == 'tanh':
            cost = tanhCost(np.asarray(y[i%samples]),net,gamma)
        #After an Epoch
        if i % samples == 0 and i!=0:   #Step5
            cost_list.append(cost)
            error = np.mean(cost_list)
            print('Epoch %d / %d - Error: %f' % (len(cost_list), int(max_iter/samples), error))
            print('w:', w)
            print('b:', b)
            if error <= eps:  
                return w,b, cost_list
        if actfunc == 'sgn':
            w,b = DeltaUpdate(w,b,np.asarray(x[i%samples]),np.asarray(y[i%samples]),net, learning_rate)
        elif actfunc == 'tanh':
            w,b
        w,b = tanhUpdate(w,b,np.asarray(x[i%samples]),np.asarray(y[i%samples]),h,learning_rate, gamma) #Step 4
    return w,b, cost_list                #Step 6

x, y = PrepareData(Class1,Class2) # Prepare Dataset

pd.DataFrame(x)

pd.DataFrame(y)

"""### Train and Plot Results"""

# define hyperparameters
max_iter = 10000
learning_rates = [0.01, 0.05, 0.1]
actfuncs = ['sgn', 'tanh']

# create subplots
fig, axs = plt.subplots(len(learning_rates), len(actfuncs), figsize=(12, 10))

# set the figure title
fig.suptitle('Loss Function Curve for Adaline Training', fontsize=16)

# define empty lists for weights and biases
weights = []
biases = []

# iterate over hyperparameters and plot the error curves
for i, lr in enumerate(learning_rates):
    for j, actfunc in enumerate(actfuncs):
        w, b, error_lst = Adaline(x, y, max_iter=max_iter, learning_rate=lr, actfunc=actfunc, samples=200)
        itr = range(1, len(error_lst) + 1)
        axs[i, j].plot(itr, error_lst, label='Loss for ' f'lr={lr}, actfunc={actfunc}')

        # add axis labels
        axs[i, j].set_title(f'lr={lr}, actfunc={actfunc}', fontsize=12)
        axs[i, j].set_xlabel('Epochs')
        axs[i, j].set_ylabel('Loss')

        # add grid
        axs[i, j].grid(True)

        # adjust tick label font size
        axs[i, j].tick_params(axis='both', which='major', labelsize=10)

        # add legend
        axs[i, j].legend(loc='upper right')

        # save weights and biases
        weights.append(w)
        biases.append(b)

        # print weights and bias
        print(f'wi: {w}x + {b}')

# adjust subplot spacing
fig.tight_layout()

# save the figure
plt.savefig("Q22figure2.pdf")

# show the plot
plt.show()

# create subplots for decision boundaries
fig2, axs2 = plt.subplots(3, 2, figsize=(12, 8))

# set the figure title
fig2.suptitle('Decision Boundaries for Adaline with Different Hyperparameters', fontsize=16)


# plot the weight and bias separation lines

for i, (w, b) in enumerate(zip(weights, biases)):
    px1, px2 = SeparationLine(-3, 3, w, b)

    plt.subplot(3, 2, i + 1)
    plt.scatter(Class1_x, Class1_y, c="red", linewidths=2)
    plt.scatter(Class2_x, Class2_y, c="blue", linewidths=2)
    plt.plot(px1, px2)

    plt.xlim(-1, 2)
    plt.ylim(-2, 2)
    plt.xlabel("x")
    plt.ylabel("y")
    plt.legend(["Class1", "Class2"], loc='upper left')

    plt.title(f"Separation line for w{i+1} and b{i+1}", fontsize=10)

plt.tight_layout()
plt.savefig("Q22figure3.pdf")
plt.show()

"""## 2.1.C Other Data Generation and Plot"""

np.random.seed(74)

avg_x_1 = 0
sd_x_1 = 0.4
Class1_x = np.random.normal(avg_x_1, sd_x_1, 100)

avg_y_1 = 0
sd_y_1 = 0.4
Class1_y = np.random.normal(avg_y_1, sd_y_1, 100)

Class1_Data = list(zip(Class1_x,Class1_y))
Class1_Label = np.ones(100)
Class1 = list(zip(Class1_Data,Class1_Label))

avg_x_2 = 1
sd_x_2 = 0.3
Class2_x = np.random.normal(avg_x_2, sd_x_2, 100)

avg_y_2 = 1
sd_y_2 = 0.3
Class2_y = np.random.normal(avg_y_2, sd_y_2, 100)

Class2_Data = list(zip(Class2_x,Class2_y))
Class2_Label = np.zeros(100) - np.ones(100)
Class2 = list(zip(Class2_Data,Class2_Label))

plt.scatter(Class1_x, Class1_y, c ="red", linewidths = 1)
plt.scatter(Class2_x, Class2_y, c ="blue", linewidths = 1)

plt.xlabel("x")
plt.ylabel("y")
plt.legend(["Class 1" , "Class 2"])
plt.grid(True)  # add grid
plt.savefig("Q23figure1.pdf")
plt.show()

x, y = PrepareData(Class1,Class2) # Prepare Dataset

pd.DataFrame(x)

pd.DataFrame(y)

"""### Train and Plot Results"""

# define hyperparameters
max_iter = 10000
learning_rates = [0.01, 0.05, 0.1]
actfuncs = ['sgn', 'tanh']

# create subplots
fig, axs = plt.subplots(len(learning_rates), len(actfuncs), figsize=(12, 10))

# set the figure title
fig.suptitle('Loss Function Curve for Adaline Training', fontsize=16)

# define empty lists for weights and biases
weights = []
biases = []

# iterate over hyperparameters and plot the error curves
for i, lr in enumerate(learning_rates):
    for j, actfunc in enumerate(actfuncs):
        w, b, error_lst = Adaline(x, y, max_iter=max_iter, learning_rate=lr, actfunc=actfunc, samples=200)
        itr = range(1, len(error_lst) + 1)
        axs[i, j].plot(itr, error_lst, label='Loss for ' f'lr={lr}, actfunc={actfunc}')

        # add axis labels
        axs[i, j].set_title(f'lr={lr}, actfunc={actfunc}', fontsize=12)
        axs[i, j].set_xlabel('Epochs')
        axs[i, j].set_ylabel('Loss')

        # add grid
        axs[i, j].grid(True)

        # adjust tick label font size
        axs[i, j].tick_params(axis='both', which='major', labelsize=10)

        # add legend
        axs[i, j].legend(loc='upper right')

        # save weights and biases
        weights.append(w)
        biases.append(b)

        # print weights and bias
        print(f'wi: {w}x + {b}')

# adjust subplot spacing
fig.tight_layout()

# save the figure
plt.savefig("Q23figure2.pdf")

# show the plot
plt.show()

# create subplots for decision boundaries
fig2, axs2 = plt.subplots(3, 2, figsize=(12, 8))

# set the figure title
fig2.suptitle('Decision Boundaries for Adaline with Different Hyperparameters', fontsize=16)


# plot the weight and bias separation lines

for i, (w, b) in enumerate(zip(weights, biases)):
    px1, px2 = SeparationLine(-3, 3, w, b)

    plt.subplot(3, 2, i + 1)
    plt.scatter(Class1_x, Class1_y, c="red", linewidths=2)
    plt.scatter(Class2_x, Class2_y, c="blue", linewidths=2)
    plt.plot(px1, px2)

    plt.xlim(-1, 2)
    plt.ylim(-2, 2)
    plt.xlabel("x")
    plt.ylabel("y")
    plt.legend(["Class1", "Class2"], loc='upper left')

    plt.title(f"Separation line for w{i+1} and b{i+1}", fontsize=10)

plt.tight_layout()
plt.savefig("Q23figure3.pdf")
plt.show()

np.random.seed(100)

avg_x_1 = 0
sd_x_1 = 0.4
Class1_x = np.random.normal(avg_x_1, sd_x_1, 100)

avg_y_1 = 0
sd_y_1 = 0.4
Class1_y = np.random.normal(avg_y_1, sd_y_1, 100)

Class1_Data = list(zip(Class1_x,Class1_y))
Class1_Label = np.ones(100)
Class1 = list(zip(Class1_Data,Class1_Label))

avg_x_2 = 1
sd_x_2 = 0.3
Class2_x = np.random.normal(avg_x_2, sd_x_2, 100)

avg_y_2 = 1
sd_y_2 = 0.3
Class2_y = np.random.normal(avg_y_2, sd_y_2, 100)

Class2_Data = list(zip(Class2_x,Class2_y))
Class2_Label = np.zeros(100) - np.ones(100)
Class2 = list(zip(Class2_Data,Class2_Label))

plt.scatter(Class1_x, Class1_y, c ="red", linewidths = 1)
plt.scatter(Class2_x, Class2_y, c ="blue", linewidths = 1)

plt.xlabel("x")
plt.ylabel("y")
plt.legend(["Class 1" , "Class 2"])
plt.grid(True)  # add grid
plt.savefig("Q231figure1.pdf")
plt.show()

x, y = PrepareData(Class1,Class2) # Prepare Dataset

pd.DataFrame(x)

pd.DataFrame(y)

# define hyperparameters
max_iter = 10000
learning_rates = [0.01, 0.05, 0.1]
actfuncs = ['sgn', 'tanh']

# create subplots
fig, axs = plt.subplots(len(learning_rates), len(actfuncs), figsize=(12, 10))

# set the figure title
fig.suptitle('Loss Function Curve for Adaline Training', fontsize=16)

# define empty lists for weights and biases
weights = []
biases = []

# iterate over hyperparameters and plot the error curves
for i, lr in enumerate(learning_rates):
    for j, actfunc in enumerate(actfuncs):
        w, b, error_lst = Adaline(x, y, max_iter=max_iter, learning_rate=lr, actfunc=actfunc, samples=200)
        itr = range(1, len(error_lst) + 1)
        axs[i, j].plot(itr, error_lst, label='Loss for ' f'lr={lr}, actfunc={actfunc}')

        # add axis labels
        axs[i, j].set_title(f'lr={lr}, actfunc={actfunc}', fontsize=12)
        axs[i, j].set_xlabel('Epochs')
        axs[i, j].set_ylabel('Loss')

        # add grid
        axs[i, j].grid(True)

        # adjust tick label font size
        axs[i, j].tick_params(axis='both', which='major', labelsize=10)

        # add legend
        axs[i, j].legend(loc='upper right')

        # save weights and biases
        weights.append(w)
        biases.append(b)

        # print weights and bias
        print(f'wi: {w}x + {b}')

# adjust subplot spacing
fig.tight_layout()

# save the figure
plt.savefig("Q231figure2.pdf")

# show the plot
plt.show()

# create subplots for decision boundaries
fig2, axs2 = plt.subplots(3, 2, figsize=(12, 8))

# set the figure title
fig2.suptitle('Decision Boundaries for Adaline with Different Hyperparameters', fontsize=16)


# plot the weight and bias separation lines

for i, (w, b) in enumerate(zip(weights, biases)):
    px1, px2 = SeparationLine(-3, 3, w, b)

    plt.subplot(3, 2, i + 1)
    plt.scatter(Class1_x, Class1_y, c="red", linewidths=2)
    plt.scatter(Class2_x, Class2_y, c="blue", linewidths=2)
    plt.plot(px1, px2)

    plt.xlim(-1, 2)
    plt.ylim(-2, 2)
    plt.xlabel("x")
    plt.ylabel("y")
    plt.legend(["Class1", "Class2"], loc='upper left')

    plt.title(f"Separation line for w{i+1} and b{i+1}", fontsize=10)

plt.tight_layout()
plt.savefig("Q231figure3.pdf")
plt.show()